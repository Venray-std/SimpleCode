#### CUDA与opencl
通用并行计算。但是CUDA仅仅能够在NVIDIA的GPU硬件上运行，而OpenCL的目标是面向任何一种Massively Parallel Processor    
NVIDIA cuDNN是用于深度神经网络的GPU加速库。它强调性能、易用性和低内存开销。
#### MPI和hadoop
运行在计算机集群上的框架，但两者有不同，一般来说，hadoop是MPI的改进版。MPI是计算与存储分离，Hadoop是计算向存储迁移。在MPI中数据存储的节点和数据处理的节点往往是不同的，一般在每次计算开始时MPI需要从数据存储节点读取需要处理的数据分配给各个计算节点，然后进行数据处理，即MPI的数据存储和数据处理是分离的。   
对于处理TB级数据的数据密集型应用，大量的数据在节点间进行交换，网络通信时间将成为影响系统性能的重要因素，性能会大大降低。   
在Hadoop中有HDFS文件系统的支持，数据是分布式存储在各个节点的，计算时各节点读取存储在自己节点的数据进行处理，从而避免了大量数据在网络上的传输，实现“计算向存储的迁移”。

MPI 在消息传递模型设计上的一些经典概念。第一个概念是通讯器（communicator）。通讯器定义了一组能够互相发消息的进程。在这组进程中，每个进程会被分配一个序号，称作秩（rank），进程间显性地通过指定秩来进行通信。   
一个进程可以通过指定另一个进程的秩以及一个独一无二的消息标签（tag）来发送消息给另一个进程。接受者可以发送一个接收特定标签标记的消息的请求，然后依次处理接收到的数据。类似这样的涉及一个发送者以及一个接受者的通信被称作点对点（point-to-point）通信。   
某个进程可能需要跟所有其他进程通信。比如主进程想发一个广播给所有的从进程。在这种情况下，手动去写一个个进程点对点的信息传递就显得很笨拙。而且事实上这样会导致网络利用率低下。MPI 有专门的接口来帮我们处理这类所有进程间的集体性（collective）通信。
#### MPI的基本函数
* MPI_Init
任何MPI程序都应该首先调用该函数

```cpp
int main(int *argc,char* argv[]) 
{ 
    MPI_Init(&argc,&argv); 
}
```
* MPI_Finalize
任何MPI程序结束时，都需要调用该函数。

```cpp
MPI_Finalize() //C++ 
```

* MPI_COMM_RANK
获得当前进程的进程标识，如进程0在执行该函数时，可以获得返回值0。可以看出该函数接口有两个参数，前者为进程所在的通信域，后者为返回的进程号。通信域可以理解为给进程分组，比如有0-5这六个进程。可以通过定义通信域，来将比如[0,1,5]这三个进程分为一组，这样就可以针对该组进行“组”操作，比如规约之类的操作。

```cpp
int MPI_Comm_Rank(MPI_Comm comm, int *rank) 

#include "mpi.h"
int main(int *argc,char* argv[]) 
{ 
    int myid; 
    MPI_Init(&argc,&argv); 
    MPI_Comm_Rank(MPI_COMM_WORLD,&myid); 
    if(myid==0) 
    { 
        printf("Hello!"); 
    } 
    if(myid==1) 
    { 
        printf("Hi!"); 
    } 
    MPI_Finalize(); 
} 
```

* MPI_COMM_SIZE
获取该通信域内的总进程数

```cpp
int MPI_Comm_Size(MPI_Comm, int *size) 
```

* MPI_SEND
该函数为发送函数，用于进程间发送消息，如进程0计算得到的结果A，需要传给进程1，就需要调用该函数。

```cpp
int MPI_Send(type* buf, int count, MPI_Datatype, int dest, int tag, MPI_Comm comm) 
```
buf为你需要传递的数据的起始地址，比如你要传递一个数组A，长度是5，则buf为数组A的首地址。count即为长度，从首地址之后count个变量。datatype为变量类型，注意该位置的变量类型是MPI预定义的变量类型，比如需要传递的是C++的int型，则在此处需要传入的参数是MPI_INT，其余同理。dest为接收的进程号，即被传递信息进程的进程号。**tag为信息标志**，同为整型变量，发送和接收需要tag一致，这将可以区分同一目的地的不同消息。比如进程0给进程1分别发送了数据A和数据B，tag可分别定义成0和1，这样在进程1接收时同样设置tag0和1去接收，避免接收混乱。

* MPI_RECV
该函数为MPI的接收函数，需要和MPI_SEND成对出现。

```cpp
int MPI_Recv(type* buf, int count, MPI_Datatype, int source, int tag, MPI_Comm comm, MPI_Status *status) 
```
参数和MPI_SEND大体相同，不同的是source这一参数，这一参数标明从哪个进程接收消息。最后多一个用于返回状态信息的参数status。

* 编译，运行
mpicc使用起来和gcc一样

```cpp
mpicc pi.c -o pi
mpic++ pi.cpp -o pi
```
运行命令

```
mpirun -np num-of-proc -machinefile nodefile your-exe-program parameters-if-any
-np参数是运行的进程数，-machinefile就是一个ascii文件，其中记录的是运行程序指定的节点
```


#### 通信
MPI最基本的通信模式是在一对进程之间进行的消息收发操作：一个进程发送消息，另一个进程接收消息。这种通信方式称为点对点通信(point to point communications)。
阻塞式通信即通信过程是阻塞式的。源进程发送数据时必须等到数据发送到消息缓冲或者直接发送成功时才可返回， 目标进程必须接收到数据后才可执行下一句代码。这就是阻塞式通信。
阻塞式通信注意避免两个进程之间相互发送消息时，可能出现死锁现象
一种常见的技术就是设法使计算与通信重叠(Overlapping Communication and Computation)，非阻塞通信可以实现这一目的。
一个通用的原则就是：尽早开始通信，尽晚完成通信。 在开始通信和完成通信之间进行计算，这样通信启动得越早，完成得越晚。就有可能有更多的计算任务可以和通信重叠，也使通信可以在计算任务执行期间完成。而不需要专门的等待时间。、

```cpp
// 阻塞通信
int MPI_Send(void *buf, int count, MPI_Datatype datatype,int dest, int tag, MPI_Comm comm)
int MPI_Recv(void *buf, int count, MPI_Datatype datatype,int source, int tag, MPI_Comm comm,MPI_Status *status)

// 非阻塞通信
int MPI_Isend(const void *buf, int count, MPI_Datatype datatype, int dest, int tag,MPI_Comm comm, MPI_Request *request)
int MPI_Irecv(void *buf, int count, MPI_Datatype datatype, int source, int tag, MPI_Comm comm, MPI_Request *request)
```
MPI_Isend函数仅提交了一个消息发送请求，并立即返回；MPI系统会在后台完成消息发送；函数为该发送操作创建了一个请求，通过request变量返回；request可供之后（查询和等待）函数使用。
MPI_Irecv函数仅提交了一个消息接收请求，并立即返回；MPI系统会在后台完成消息发送；函数为该接收操作创建了一个请求，通过request变量返回；request可供之后查询和等待函数使用

```cpp
int MPI_Wait(MPI_Request *request, MPI_Status *status)
int MPI_Test(MPI_Request *request,int *flag, MPI_Status *status)
```
等待&检测一个通信请求的完成。**MPI_Wait 阻塞等待通信函数完成后返回； MPI_Test检测某通信，不论其是否完成，都立刻返回**。如果通信完成，则flag=true；


```cpp
/*阻塞通信*/
MPI_Comm_rank(MPI_COMM_WORLD, &myRank); /* find process rank */
if (myRank == 0) {
    int x=10;
    MPI_Send(&x, 1, MPI_INT, 1, 0, MPI_COMM_WORLD);
} 
else if (myRank == 1) {
    int x;
    MPI_Recv(&x, 1, MPI_INT, 0, MPI_ANY_TAG, MPI_COMM_WORLD, status);
}

/* 非阻塞通信 */
MPI_Comm_rank(MPI_COMM_WORLD, &myrank);/* find process rank */
if (myrank == 0) {
    int x=10;
    MPI_Isend(&x, 1, MPI_INT, 1, 0, MPI_COMM_WORLD, req1);
    compute();
} 
else if (myrank == 1) {
    int x;
    MPI_Irecv(&x, 1, MPI_INT, 0, MPI_ANY_TAG, MPI_COMM_WORLD, req1);
}
MPI_Wait(req1, status);
```

非阻塞发送注意事项
* 非阻塞MPI_Isend调用后不要马上修改发送buf，否则可能真正的发送的是buf中的新内容

```cpp
*buf =3;
MPI_Isend(buf, 1, MPI_INT …)
*buf = 4;   //不能确定接收者接收到3还是4，或者其他啥内容
MPI_Wait(…);
```
* 通过非阻塞接口，可以进行连续不断的数据计算和通信，

```cpp
	int rank, data[100];
	MPI_Init(0, 0);
	MPI_Comm_rank(MPI_COMM_WORLD, &rank);

	if (rank == 0) 
	{
		MPI_Request request[100];
		//连续异步发送数据
		for (int i=0; i< 100; i++) 
		{
			data[i] = i + 1;
			//异步发送数据, 不用等待对方接收，就进行下一个数据的计算和发送
			MPI_Isend(&data[i], 1, MPI_INT, 1, 0, MPI_COMM_WORLD, &request[i]);
			printf("process %d send data[%d]\n", rank, i);
		}
		//在多个request上等待
		MPI_Waitall(100, request, MPI_STATUSES_IGNORE);
	}
	else 
	{
		for (int i = 0; i < 100; i++)
		{
			//同步接收数据
			MPI_Recv(&data[i], 1, MPI_INT, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
			printf("process %d receive data[%d]\n", rank, i);
		}	
	}

	MPI_Finalize();
```

### 并行程序设计模式
两种基本的并行程序设计模式，对等模式和主从模式。在对等模式中，每个进程的功能和代码基本一致，只是处理的数据和对象有所不同。而主从模式中，会有一个 master 线程，用来管理其他的线程（称为 workers 或者 slaves）。

#### 对等模式
对等模式中，各个进程的功能基本相同，因此用SPMD（单程序多数据）程序可以更好的表达。在讲解对等模式时，常用Jacobi迭代作为范例。Jacobi迭代就是用上下左右周围的四个点取平均值来得到新的点。
如果两个进程都在等待对方发送消息，这样程序执行到这一步时就会卡住。如果两个进程都在给对方进程发送，而此时两个进程都没有接收指令，也同样会出现问题。因此，在使用SEND和RECV的时候，为了保证程序的安全性，需要匹配SEND和RECV。

以Jacobi迭代为例，

```cpp
// 这里引入中间变量b[i][j]，使a不依赖，方便并行
for (i = 1; i < n - 1; i++) {
    for (j = 1; j < m - 1; j++) {
        b[i][j] = 0.25 * (a[i - 1][j] + a[i + 1][j] + a[i][j + 1] + a[i][j - 1]);
    }
}

for (i = 1; i < m - 1; i++) {
    for (j = 1; j < n - 1; j++) {
        a[i][j] = b[i][j];
    }
}
```

#### 主从模式
主从模式。在逻辑上规定一个主进程，用于将数据发送给各个进程，再收集各个进程所计算的结果。
主进程主要干了三件事，定义数据、发送数据和接收计算结果。